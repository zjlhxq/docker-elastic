version: "3.9"

# 10 Things to Consider When Planning Your Elasticsearch Project: https://ecmarchitect.com/archives/2015/07/27/4031
# Using Apache JMeter to Test Elasticsearch: https://ecmarchitect.com/archives/2014/09/02/3915

services:

  elasticsearch:
    image: ${DOCKER_REPO:-}elastic/elasticsearch:${ELASTIC_VERSION:-8.3.3}
    environment:
      # https://github.com/docker/swarmkit/issues/1951
      - node.name={{.Node.Hostname}}
      - discovery.seed_hosts=${INITIAL_MASTER_NODES:-ccm-l2-kafka-node1}
      - cluster.initial_master_nodes=${INITIAL_MASTER_NODES:-ccm-l2-kafka-node1}
      - cluster.name=elk
      - ELASTIC_PASSWORD=${ELASTICSEARCH_PASSWORD:-changeme}
      - xpack.security.enabled=true
      - xpack.monitoring.collection.enabled=true
      - xpack.security.audit.enabled=false
      - xpack.license.self_generated.type=basic
      - network.host=0.0.0.0
      - "ES_JAVA_OPTS=-Xms${ES_HEAP_SIZE:-2g} -Xmx${ES_HEAP_SIZE:-2g}"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    networks:
      - elk_network
    ports:
      - 9200:9200
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - elasticsearch:/usr/share/elasticsearch/data
    deploy:
      placement:
        max_replicas_per_node: 1
        constraints:
        - node.labels.ElasticNode == ccm-elastic-node
      replicas: 3
      update_config:
        parallelism: 1
        delay: 10s
      restart_policy:
        condition: on-failure
    logging:
      driver: "json-file"
      options:
        max-file: "5"    # number of files or file count
        max-size: "200m" # file size

  logstash:
    image: ${DOCKER_REPO:-}elastic/logstash:${ELASTIC_VERSION:-8.3.3}
    hostname: "{{.Node.Hostname}}-logstash"
    environment:
      - XPACK_MONITORING_ELASTICSEARCH_URL=http://elasticsearch:9200
      - XPACK_MONITORING_ELASTICSEARCH_USERNAME=${ELASTICSEARCH_USERNAME:-elastic}
      - XPACK_MONITORING_ELASTICSEARCH_PASSWORD=${ELASTICSEARCH_PASSWORD:-changeme}
    ports:
      - "12201:12201/udp"
    networks:
      - elk_network
    configs:
      - source: ls_config
        target: /usr/share/logstash/pipeline/logstash.conf
    deploy:
      placement:
        max_replicas_per_node: 1
        constraints:
        - node.hostname == ccm-l2-kafka-node1
      replicas: 1
      update_config:
        parallelism: 1
        delay: 10s
      restart_policy:
        condition: on-failure
    logging:
      driver: "json-file"
      options:
        max-file: "5"    # number of files or file count
        max-size: "200m" # file size


  kibana:
    image: ${DOCKER_REPO:-}elastic/kibana:${ELASTIC_VERSION:-8.3.3}
    hostname: "{{.Node.Hostname}}-kibana"
    environment:
      - ELASTICSEARCH_URL=http://elasticsearch:9200
      - ELASTICSEARCH_USERNAME=${ELASTICSEARCH_USERNAME:-elastic}
      - ELASTICSEARCH_PASSWORD=${ELASTICSEARCH_PASSWORD:-changeme}
      - SERVER_NAME="{{.Node.Hostname}}-kibana"
    networks:
      - elk_network
    ports:
      - target: 5601
        published: 5601
        protocol: tcp
        mode: host
    volumes:
      - kibana:/usr/share/kibana/data
    deploy:
      placement:
        max_replicas_per_node: 1
        constraints:
        - node.hostname == ccm-l2-kafka-node2
      replicas: 1
      update_config:
        parallelism: 1
        delay: 10s
      restart_policy:
        condition: on-failure
    logging:
      driver: "json-file"
      options:
        max-file: "5"    # number of files or file count
        max-size: "200m" # file size

  apm-server:
    image: ${DOCKER_REPO:-}elastic/apm-server:${ELASTIC_VERSION:-8.3.3}
    hostname: "{{.Node.Hostname}}-apm-server"
    networks:
      - elk_network
    command: > 
        --strict.perms=false -e
        -E apm-server.rum.enabled=true
        -E setup.kibana.host=kibana:5601
        -E setup.kibana.username=${ELASTICSEARCH_USERNAME}
        -E setup.kibana.password=${ELASTICSEARCH_PASSWORD}
        -E setup.template.settings.index.number_of_replicas=0
        -E apm-server.kibana.enabled=true
        -E apm-server.kibana.host=kibana:5601
        -E apm-server.kibana.username=${ELASTICSEARCH_USERNAME}
        -E apm-server.kibana.password=${ELASTICSEARCH_PASSWORD}
        -E output.elasticsearch.hosts=["elasticsearch:9200"]
        -E output.elasticsearch.username=${ELASTICSEARCH_USERNAME}
        -E output.elasticsearch.password=${ELASTICSEARCH_PASSWORD}
        -E xpack.monitoring.enabled=false
    deploy:
      labels:
        - com.df.notify=true
        - com.df.distribute=true
        - com.df.servicePath=/
        - com.df.port=8200
        - com.df.srcPort=8200
      placement:
        max_replicas_per_node: 1
        constraints:
        - node.hostname == ccm-l2-kafka-node3
      replicas: 1
      update_config:
        parallelism: 1
        delay: 10s
      restart_policy:
        condition: on-failure
    logging:
      driver: "json-file"
      options:
        max-file: "5"    # number of files or file count
        max-size: "200m" # file size

  # How to Tune Elastic Beats Performance: A Practical Example with Batch Size, Worker Count, and More
  # https://www.elastic.co/blog/how-to-tune-elastic-beats-performance-a-practical-example-with-batch-size-worker-count-and-more?blade=tw&hulk=social
  filebeat:
    image: ${DOCKER_REPO:-}elastic/filebeat:${ELASTIC_VERSION:-8.3.3}
    # https://github.com/docker/swarmkit/issues/1951
    hostname: "{{.Node.Hostname}}-filebeat"
    # Need to override user so we can access the log files, and docker.sock
    user: root
    networks:
      - elk_network
    extra_hosts:
      - "host.docker.internal:host-gateway"
    configs:
      - source: fb_config
        target: /usr/share/filebeat/filebeat.yml
    volumes:
      - filebeat:/usr/share/filebeat/data
      - /var/run/docker.sock:/var/run/docker.sock
      # This is needed for filebeat to load container log path as specified in filebeat.yml
      - /var/lib/docker/containers/:/var/lib/docker/containers/:ro

      # # This is needed for filebeat to load jenkins build log path as specified in filebeat.yml
      # - /var/lib/docker/volumes/jenkins_home/_data/jobs/:/var/lib/docker/volumes/jenkins_home/_data/jobs/:ro

      # This is needed for filebeat to load logs for system and auth modules
      - /var/log/:/var/log/:ro
      # This is needed for filebeat to load logs for auditd module. you might have to install audit system
      # on ubuntu first (sudo apt-get install -y auditd audispd-plugins)
      #- /var/log/audit/:/var/log/audit/:ro
    environment:
      - ELASTICSEARCH_HOST=${ELASTICSEARCH_HOST:-nextgen}
      - KIBANA_HOST=${KIBANA_HOST:-nextgen}
      - ELASTICSEARCH_USERNAME=${ELASTICSEARCH_USERNAME:-elastic}
      - ELASTICSEARCH_PASSWORD=${ELASTICSEARCH_PASSWORD:-changeme}
    # disable strict permission checks
    command: ["--strict.perms=false"]
    deploy:
      mode: global
    logging:
      driver: "json-file"
      options:
        max-file: "5"    # number of files or file count
        max-size: "200m" # file size

networks:
  elk_network:
    external: true

volumes:
  elasticsearch:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: /kafka/elasticsearch
  kibana:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: /kafka/kibana
 
  filebeat:

configs:
  ls_config:
    file: $PWD/elk/logstash/config/pipeline/logstash.conf
  fb_config:
    file: $PWD/elk/beats/filebeat/config/filebeat.yml



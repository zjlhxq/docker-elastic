version: "3.9"

# 10 Things to Consider When Planning Your Elasticsearch Project: https://ecmarchitect.com/archives/2015/07/27/4031
# Using Apache JMeter to Test Elasticsearch: https://ecmarchitect.com/archives/2014/09/02/3915

services:

  elasticsearch_server:
    image: ${DOCKER_REPO:-}elastic/elasticsearch:${ELASTIC_VERSION:-7.17.5}
    hostname: "{{.Node.Hostname}}"
    #    user: "${UID}:0"
    environment:
      # https://github.com/docker/swarmkit/issues/1951
      - node.name={{.Node.Hostname}}
      - discovery.seed_hosts=${INITIAL_MASTER_NODES:-ccm-l2-kafka-node1}
      - cluster.initial_master_nodes=${INITIAL_MASTER_NODES:-ccm-l2-kafka-node1}
      - cluster.name=elk
      - ELASTIC_PASSWORD=${ELASTICSEARCH_PASSWORD:-changeme}
      - xpack.security.enabled=false
      - xpack.monitoring.collection.enabled=true
        #- xpack.security.audit.enabled=false
      - xpack.license.self_generated.type=basic
        #- xpack.security.transport.ssl.enabled=true
      - network.host=0.0.0.0
      - "ES_JAVA_OPTS=-Xms${ES_HEAP_SIZE:-2g} -Xmx${ES_HEAP_SIZE:-2g}"
        #      - TAKE_FILE_OWNERSHIP="true"
        #    command: /bin/bash -c 'chmod 1777 /tmp'
    ulimits:
      memlock:
        soft: -1
        hard: -1
    networks:
      - elk_network
    extra_hosts:
      - "host.docker.internal:host-gateway"
      - "ccm-l2-kafka-node1:10.68.22.60"
      - "ccm-l2-kafka-node2:10.68.22.71"
      - "ccm-l2-kafka-node3:10.68.22.62"

    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
      - elasticsearch-config:/usr/share/elasticsearch/config
    deploy:
      placement:
        max_replicas_per_node: 1
        constraints:
        - node.labels.ElasticNode == ccm-elastic-node
      replicas: 3
      update_config:
        parallelism: 1
        delay: 10s
      restart_policy:
        condition: on-failure
    logging:
      driver: "json-file"
      options:
        max-file: "5"    # number of files or file count
        max-size: "200m" # file size

        #  logstash:
        #    image: ${DOCKER_REPO:-}elastic/logstash:${ELASTIC_VERSION:-7.17.5}
        #    hostname: "{{.Node.Hostname}}-logstash"
        #    environment:
        #      - XPACK_MONITORING_ELASTICSEARCH_URL=http://ccm-l2-kafka-node1:9200
        #      - XPACK_MONITORING_ELASTICSEARCH_USERNAME=${ELASTICSEARCH_USERNAME:-elastic}
        #      - XPACK_MONITORING_ELASTICSEARCH_PASSWORD=${ELASTICSEARCH_PASSWORD:-changeme}
        #    networks:
        #      - elk_network
        #    extra_hosts:
        #      - "host.docker.internal:host-gateway"
        #      - "ccm-l2-kafka-node1:10.68.22.60"
        #      - "ccm-l2-kafka-node2:10.68.22.71"
        #      - "ccm-l2-kafka-node3:10.68.22.62"
        #    configs:
        #      - source: ls_config
        #        target: /usr/share/logstash/pipeline/logstash.conf
        #    deploy:
        #      placement:
        #        max_replicas_per_node: 1
        #        constraints:
        #        - node.hostname == ccm-l2-kafka-node1
        #      replicas: 1
        #      update_config:
        #        parallelism: 1
        #        delay: 10s
        #      restart_policy:
        #        condition: on-failure
        #    logging:
        #      driver: "json-file"
        #      options:
        #        max-file: "5"    # number of files or file count
        #        max-size: "200m" # file size


  kibana:
    image: ${DOCKER_REPO:-}elastic/kibana:${ELASTIC_VERSION:-7.17.5}
    hostname: "{{.Node.Hostname}}-kibana"
    environment:
            #- ELASTICSEARCH_URL=http://ccm-l2-kafka-node2:9200
      - ELASTICSEARCH_HOSTS=["http://ccm-l2-kafka-node2:9200"]
      - ELASTICSEARCH_USERNAME=${ELASTICSEARCH_USERNAME:-elastic}
      - ELASTICSEARCH_PASSWORD=${ELASTICSEARCH_PASSWORD:-changeme}
      - SERVER_NAME="{{.Node.Hostname}}-kibana"
        #- xpack.encryptedSavedObjects.encryptionKey="73357638792F423F4528482B4D625165"
    networks:
      - elk_network
    extra_hosts:
      - "host.docker.internal:host-gateway"
      - "ccm-l2-kafka-node1:10.68.22.60"
      - "ccm-l2-kafka-node2:10.68.22.71"
      - "ccm-l2-kafka-node3:10.68.22.62"
    volumes:
      - kibana:/usr/share/kibana/data
    deploy:
      placement:
        max_replicas_per_node: 1
        constraints:
        - node.hostname == ccm-l2-kafka-node2
      replicas: 1
      update_config:
        parallelism: 1
        delay: 10s
      restart_policy:
        condition: on-failure
    logging:
      driver: "json-file"
      options:
        max-file: "5"    # number of files or file count
        max-size: "200m" # file size

  apm-server:
    image: ${DOCKER_REPO:-}elastic/apm-server:${ELASTIC_VERSION:-7.17.5}
    hostname: "{{.Node.Hostname}}-apm-server"
    networks:
      - elk_network
    command: > 
        --strict.perms=false -e
        -E apm-server.rum.enabled=true
        -E setup.kibana.host=${KIBANA_HOST}:5601
        -E setup.kibana.username=${ELASTICSEARCH_USERNAME}
        -E setup.kibana.password=${ELASTICSEARCH_PASSWORD}
        -E setup.template.settings.index.number_of_replicas=0
        -E apm-server.kibana.enabled=true
        -E apm-server.kibana.host=${KIBANA_HOST}:5601
        -E apm-server.kibana.username=${ELASTICSEARCH_USERNAME}
        -E apm-server.kibana.password=${ELASTICSEARCH_PASSWORD}
        -E output.elasticsearch.hosts=["${ELASTICSEARCH_HOST}:9200"]
        -E output.elasticsearch.username=${ELASTICSEARCH_USERNAME}
        -E output.elasticsearch.password=${ELASTICSEARCH_PASSWORD}
        -E xpack.monitoring.enabled=false
    deploy:
      labels:
        - com.df.notify=true
        - com.df.distribute=true
        - com.df.servicePath=/
        - com.df.port=8200
        - com.df.srcPort=8200
      placement:
        max_replicas_per_node: 1
        constraints:
        - node.hostname == ccm-l2-kafka-node3
      replicas: 1
      update_config:
        parallelism: 1
        delay: 10s
      restart_policy:
        condition: on-failure
    logging:
      driver: "json-file"
      options:
        max-file: "5"    # number of files or file count
        max-size: "200m" # file size

  # How to Tune Elastic Beats Performance: A Practical Example with Batch Size, Worker Count, and More
  # https://www.elastic.co/blog/how-to-tune-elastic-beats-performance-a-practical-example-with-batch-size-worker-count-and-more?blade=tw&hulk=social
  filebeat:
    image: ${DOCKER_REPO:-}elastic/filebeat:${ELASTIC_VERSION:-7.17.5}
    # https://github.com/docker/swarmkit/issues/1951
    hostname: "{{.Node.Hostname}}-filebeat"
    # Need to override user so we can access the log files, and docker.sock
    user: root
    networks:
      - elk_network
    extra_hosts:
      - "host.docker.internal:host-gateway"
      - "ccm-l2-kafka-node1:10.68.22.60"
      - "ccm-l2-kafka-node2:10.68.22.71"
      - "ccm-l2-kafka-node3:10.68.22.62"
    configs:
      - source: fb_config
        target: /usr/share/filebeat/filebeat.yml
    volumes:
      - filebeat:/usr/share/filebeat/data
      - /var/run/docker.sock:/var/run/docker.sock
      # This is needed for filebeat to load container log path as specified in filebeat.yml
      - /data/docker/containers/:/var/lib/docker/containers/:ro

      # # This is needed for filebeat to load jenkins build log path as specified in filebeat.yml
      # - /var/lib/docker/volumes/jenkins_home/_data/jobs/:/var/lib/docker/volumes/jenkins_home/_data/jobs/:ro

      # This is needed for filebeat to load logs for system and auth modules
      - /var/log/:/var/log/:ro
      # This is needed for filebeat to load logs for auditd module. you might have to install audit system
      # on ubuntu first (sudo apt-get install -y auditd audispd-plugins)
      #- /var/log/audit/:/var/log/audit/:ro
    environment:
      - ELASTICSEARCH_HOST=${ELASTICSEARCH_HOST:-nextgen}
      - KIBANA_HOST=${KIBANA_HOST:-nextgen}
      - ELASTICSEARCH_USERNAME=${ELASTICSEARCH_USERNAME:-elastic}
      - ELASTICSEARCH_PASSWORD=${ELASTICSEARCH_PASSWORD:-changeme}
    # disable strict permission checks
    command: ["-e","--strict.perms=false"]
    deploy:
      mode: global
      update_config:
        parallelism: 1
        delay: 10s
      restart_policy:
        condition: on-failure
    logging:
      driver: "json-file"
      options:
        max-file: "5"    # number of files or file count
        max-size: "200m" # file size

networks:
  elk_network:
    external: true
    name: host

volumes:
  elasticsearch-data:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: /kafka/elasticsearch/data
  elasticsearch-config:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: /kafka/elasticsearch/config
  kibana:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: /kafka/kibana
 
  filebeat:

configs:
  ls_config:
    file: $PWD/elk/logstash/config/pipeline/logstash.conf
  fb_config:
    file: $PWD/elk/beats/filebeat/config/filebeat.yml




